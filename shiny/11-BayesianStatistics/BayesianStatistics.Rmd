---
title: "Bayesian Statistics"
output: 
  learnr::tutorial:
    progressive: true
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(assertthat)
library(ggplot2)
library(BayesFactor)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.eval = FALSE)
knitr::opts_chunk$set(exercise.checker = gradethis::grade_learnr)

```

## Bayesian statistics

The goal of Bayesian statistics is to infer the features of the process that gives rise to data, using the data along with any other knowledge that we have.   In this tutorial we will walk through a very simple example of how to perform Bayesian inference, in which we ask ask whether a coin is fair.  

### Setting up the prior

We first need to come up with a *prior distribution*, which describes our beliefs about the underlying process that generates the data, prior to actually seeing any data.  For this example, we are going to limit ourselves to 9 possible values of $P(heads)$, from 0.1 to 0.9 in steps of 0.1.  We will start with a *uniform prior*, which says that the probability of each possible value of $P(heads)$ is equal, which in this case comes out to 1/9.  Let's create a data frame containing the $P(heads)$ values and their respective prior probabilities.

```{r phead_setup}

flip_coins <- function(nflips, pheads){
  flips = runif(nflips)
  return(sum(flips < pheads))
}

set.seed(7)
nflips <- 20
pheads_true <- 0.5
nheads_observed <- flip_coins(nflips, pheads_true)

dice_df <- data.frame(pheads=seq(0.1, 0.9, 0.1),
                      flat_prior = 1/9) %>%
  mutate(likelihood = dbinom(nheads_observed, nflips, pheads))

marginal_likelihood_flat <- sum(dice_df$flat_prior * dice_df$likelihood)

dice_df <- mutate(dice_df,
                  posterior_flat = (likelihood * flat_prior)/marginal_likelihood_flat,
                  prior_95 = dbinom(95, 200, pheads))

marginal_likelihood_95 <- sum(dice_df$prior_95 * dice_df$likelihood)
dice_df <- mutate(dice_df,
                  posterior_95 = (likelihood * prior_95)/marginal_likelihood_95)
bf_result <- proportionBF(14, 20, 0.5)

```

### Collect the data

Now let's flip the coin 20 times. We can use a custom function called `flip_coins()` to do this.  It takes two arguments:

- *nflips*: the number of coin flips
- *pheads*: the true probability of heads -- in this case, we will assume that the coin 

It returns the number of heads in the sample.  

```{r data, exercise=TRUE, exercise.setup='phead_setup'}
set.seed(7)
nflips <- 20
pheads_true <- 0.5
nheads_observed <- flip_coins(nflips, pheads_true)
nheads_observed
```

### Compute the likelihood

Now we need to compute the likelihood of the observed data under each of the possible values of $P(heads)$.    Because each coin flip is a Bernoulli event, we can describe this using the binomial distribution; in particular, the `dbinom()` function tells us the likelihood of a particular number of heads out of a particular number of flips given  a particular value of $P(heads)$.

```{r like, exercise=TRUE, exercise.setup='phead_setup'}

dice_df <- mutate(dice_df,
                  likelihood = dbinom(nheads_observed, nflips, pheads))
```

### Compute the marginal likelihood

The marginal likelihood is necessary in order to make sure that our posterior distribution is a proper probability distribution -- that is, that all of the values fall in [0, 1] and sum to one.  We compute this by simply multiplying the prior by the likelihood for each value of $P(heads)$ and adding them all together.

```{r mlflat, exercise=TRUE, exercise.setup='phead_setup'}
marginal_likelihood_flat <- sum(dice_df$flat_prior * dice_df$likelihood)
```

### Compute the posterior

We can now use Bayes's theorem to compute posterior distribution.

```{r postflat, exercise=TRUE, exercise.setup='phead_setup'}
dice_df <- mutate(dice_df,
                  posterior_flat = (likelihood * flat_prior)/marginal_likelihood_flat)
```


Plot the posterior probability *posterior_flat* for each value of *pheads* using a bar plot

```{r plotflatpost, exercise=TRUE, exercise.setup='phead_setup'}

ggplot(dice_df, aes(x=pheads, y=posterior_flat)) + 
  geom_bar(stat='identity') + 
  scale_x_continuous(breaks=seq(0.1, 0.9, 0.1))
```

Here we see that the maximum posterior value is the same as the observed proportion of heads.  This is because we used a flat prior.  Let's say that instead we had previously flipped the same coin 200 times, and came out with 95 heads.  Given these prior data, we can use a binomial distribution for 95 successes out of 200 throws as our prior:

```{r prior95, exercise=TRUE, exercise.setup='phead_setup'}
dice_df <- mutate(dice_df,
                  prior_95 = dbinom(95, 200, pheads))


```


We can then perform the remainder of our computations to obtain the posterior distribution:

```{r post95, exercise=TRUE, exercise.setup='phead_setup'}
marginal_likelihood_95 <- sum(dice_df$prior_95 * dice_df$likelihood)
dice_df <- mutate(dice_df,
                  posterior_95 = (likelihood * prior_95)/marginal_likelihood_95)

ggplot(dice_df, aes(x=pheads, y=posterior_95)) + 
  geom_bar(stat='identity') + 
  scale_x_continuous(breaks=seq(0.1, 0.9, 0.1))

```

Now we see that the same data lead us to a very different posterior distribution, with the most likely value of $P(heads)$ now being 0.5.  This is because our prior data were based on many more observations, and thus provide a much more precise estimate compared to our smaller new sample. Bayes theorem provides us the means to combine these data into a single estimate.

## Bayes factors

The Bayes factor allows us to express the relative evidence for or against one particular hypothesis compared to another. We can compute Bayes factors using the *BayesFactor* library for R.  Let's compute the Bayes factor for our example from the previous page, in which we rolled 14 heads out of 20 rolls.  We can use the `proportionBF()` function to compute the Bayes factor in favor of the alternative hypothesis that the probability is different from 0.5, given these data:

```{r bf, exercise=TRUE, exercise.setup='phead_setup'}

bf_result <- proportionBF(14, 20, 0.5)

bf_result
```


Here we see that the Bayes factor is about 1.7, which means that the evidence (along with the prior, which is determined by the `proportionBF()` function) favors the alternative hypothesis that $P(heads) \ne 0.5$.  However, this Bayes factor is very weak; according to the guidelines presented in the textbook, it would not even warrant any mention.

#### Exercise

Compute the Bayes factor for 70 heads out of 200 coin flips, given $P(heads) = 0.5$.  


```{r bf200, exercise=TRUE}
bf_result <- proportionBF(70, 200, 0.5)
bf_result
```


```{r bf-mc, echo=FALSE}
question("How would you interpret this Bayes factor?",
  answer("negligible evidence for the alternative hypothesis"),
  answer("positive evidence for the alternative hypothesis"),
  answer("strong evidence for the alternative hypothesis"),
  answer("very strong evidence for the alternative hypothesis", correct=TRUE)
)
```

