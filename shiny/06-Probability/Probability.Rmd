---
title: "Probability"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(ggplot2)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.eval = FALSE)
knitr::opts_chunk$set(exercise.checker = gradethis::grade_learnr)
```

## Empirical probability

Let's use our survey data to compute the probability that someone in the class has taken a statistics class before.  First, let's look at the data to remind ourselves of the content:

```{r survey-setup, echo=FALSE, message=FALSE, warning=FALSE}
load_cleaned_survey_data <- function(){
  survey_data <- read_csv('https://raw.githubusercontent.com/poldrack/learnr_demos/master/data/surveydata.csv')
  survey_data_filtered <- filter(survey_data,
                                         year %in% c('1', '2', '3', '4'))
  survey_data_filtered_dropna <- drop_na(survey_data_filtered)
}

survey_data <- load_cleaned_survey_data()
survey_data_by_year <- group_by(survey_data, year)
year4_and_statsbefore <- survey_data$year == "4" & survey_data$stats_before
joint_prob <- mean(year4_and_statsbefore)
p_year4 <- mean(survey_data$year == "4")

```

```{r p_survey, exercise=TRUE, exercise.setup='survey-setup', message=FALSE, warning=FALSE}
survey_data <- load_cleaned_survey_data()
glimpse(survey_data)
```

The variable *stats_before* contains logical values denoting whether the student has had a previous statistics class (TRUE) or not (FALSE).  We want to determine the probability that a student in this group has taken statistics before - that is, the probability that this variable takes the value TRUE.

```{r probcompute_question}
question("Which of the following would correctly compute the probability? Choose all that apply.",
  answer("sum(survey_data$stats_before)/length(survey_data$stats_before)", correct=TRUE),
  answer("mean(survey_data$stats_before)", correct=TRUE),
  answer("mean(survey_data$stats_before == 1)", correct=TRUE),
  answer("sum(survey_data$stats_before/length(survey_data$stats_before))", correct=TRUE),
  random_answer_order = TRUE
)

```

There are two essential facts that one needs to understand in order to make sense of the outcomes on that question.  First, R treats the value TRUE and number 1 as equivalent:

```{r true_one, exercise=TRUE}

TRUE == 1
```

Second, there is a fundamental similarity between the computation of a probability and the computation of the mean.  Here is the equation for computing an empirical probability:

$$
P = \frac{\text{# of events observed}}{\text{# of events counted}}
$$
For example, in the book we computed the probability of rain in San Francisco by dividing the number of days with rain in one year by the total number of days in the year.

Now let's look at the formula for the mean:

$$
\bar{X} = \frac{\sum_1^N{x_i}}{N}
$$

In this case, the $x_i$ are the observed values, and N is the total number of values.  If the $x_i$ take values of one (when the event happens) or zero (when the event doesn't happen), then the top of this equation is just computing the number of times the event happens, and $N$ is the total number of observations -- which is exactly the equation for computing the probability!

#### Exercise

Use the `mean()` function to compute the probability of having taken a statistics class before.

```{r mean_prog, exercise=TRUE, exercise.setup='survey-setup'}

p_stats_before <- ...
p_stats_before

```

```{r mean_prog-solution, warning=FALSE}
p_stats_before <- mean(survey_data$stats_before)
p_stats_before
```

```{r mean_prog-check}
grade_code(incorrect='Try again...')
```

## Conditional probability

Conditional probability is the probability of some event occurring, given that some other event has occurred. We can compute this as follows:

$$
P(A|B) = \frac{P(A \cap B)}{P(B)}
$$
That is, we count occurrences of A *and* B, but then divide by the probability of B. This is equivalent to saying that we are going to count how many times A occurs, but only for the events where B occurred.

Let's compute the conditional probability of someone having taken a statistics class before, given that they are a first-year student.

$$
P(\text{stats before}|\text{year 4}) = \frac{P(\text{stats before} \cap \text{year 4})}{P(\text{year 4})}
$$
#### Exercise

Use the mean to compute compute $P(\text{year 4})$.  Here again you can take advantage of the fact that we can simply take the mean of a logical value to compute the probability -- in this case, the logical values are generated by the *==* test for whether each value is equal to the value "4" (we have to use the quotes because the year values are stored in the variable as character strings).  

```{r p_year4, exercise=TRUE, exercise.setup='survey-setup'}

p_year4 <- ...
p_year4

```

```{r p_year4-solution, warning=FALSE}
p_year4 <- mean(survey_data$year == "4")
p_year4
```

```{r p_year4-check}
grade_code(incorrect='Try again...')
```


Next, let's compute the joint probability of having taken stats before and being year 4 ($P(\text{stats before} \cap \text{year 4})$).  You might remember that in the book we discussed how one can sometimes compute the joint probability simply by multiplying the individual probabilities, but that only works if the events are independent; in this case we don't know whether or not they are independent, so we will compute the joint probability by first creating a new variable that represents the joint event (year 4 and having taken stats before).  To do this we use the $&$ operator, which returns true if the values on each side of the operator are both true:

```{r and, exercise=TRUE}

a <- 1
b <- 2

a == 1 & b == 2
```

We can applly it here to create a vector of truth values for cases where year is equal to 4 and *stats_before* is TRUE:

```{r comboprob, exercise=TRUE, exercise.setup='survey-setup'}
year4_and_statsbefore <- survey_data$year == "4" & survey_data$stats_before

year4_and_statsbefore

```

#### Exercise

First compute the joint probability ($P(\text{stats before} \cap \text{year 4})$) using that new variable *year4_and_statsbefore*:

, which is a logical value so we can simply take its mean to obtain the probability:

```{r jointprob, exercise=TRUE, exercise.setup='survey-setup'}
joint_prob <- ...
joint_prob

```

```{r jointprob-solution}
joint_prob <- mean(year4_and_statsbefore)
joint_prob
```

```{r jointprob-check}
grade_code(incorrect='Try again...')
```

Now compute the conditional probability $P(\text{stats before}|\text{year 4})$:

```{r condprob, exercise=TRUE, exercise.setup='survey-setup'}

p_statsbefore_given_year4 <- ...

p_statsbefore_given_year4
```

```{r condprob-solution}
p_statsbefore_given_year4 <- joint_prob / p_year4

p_statsbefore_given_year4

```

```{r condprob-check}
grade_code(incorrect='Try again...')
```


## Probability distributions

A probability distribution provides us with a way to compute the likelihood of some particular event, given some assumptions about how the events are generated.  

One example that we encountered in the book is the *binomial* distribution, which is appropriate for determining the likelihood of a number of outcomes on a set of trials with binary (success or failure) outcomes (known as *Bernoulli trials*). One of the most useful things we can do with a probability distribution is to ask how likely a particular set of events would be, given some assumptions about the probability of those events. Let's say that we flip a coin 10 times, and count the number of heads.  The `dbinom()` function tells us the likelihood of any particular number of heads occurring, assuming that the coin is fair.  This function takes three arguments:


- the value that we want to assess (that is, how many heads)?
- the number of trials (how many flips?)
- the probability of the event occurring (in this case, 0.5 if we assume a fair coin)

Let's say that we flip 10 coins and we get 3 heads.  How likely is this?  We can compute this using `dbinom()`:

```{r dbinom, exercise=TRUE}

dbinom(3, 10, 0.5)
```

This tells us that if we did our experiment many times, we would get a result of three heads about 11% of the time.  

#### Exercise

```{r binom_setup, echo=FALSE}
flip_df <- data.frame(nheads = seq(0, 10))
flip_df <- mutate(flip_df, 
                  probability=dbinom(nheads, 10, .5))

```

In this exercise you will compute the probability of each possible number of heads out of 10 flips with a probability of 0.5, and plot these probabilities for each value using a bar plot.  First create a data frame called *flip_df* that contains a variable called *heads* that contains all of the possible numbers of heads that could occur on ten flips (don't forget to include zero!).  Then, use `mutate()` to add a variable called *probability* that contains the results of `dbinom()` for each value of *nheads*.  Then plot these using `ggplot()`, with *nheads* as the x variable and *probability* as the y variable.  To create the bar graph, you will need to use `geom_bar(stat='identity')` since by default `geom_bar()` tries to count values rather than simply plotting the values that are present.

```{r plotdbinom, exercise=TRUE}
flip_df <- data.frame(nheads = seq(0, 10))
flip_df <- mutate(flip_df, 
                  probability=dbinom(nheads, 10, .5))
ggplot(flip_df, aes(x=nheads, y=probability)) + 
  geom_bar(stat='identity')
```

```{r plotdbinom-solution}
flip_df <- data.frame(nheads = seq(0, 10))
flip_df <- mutate(flip_df, 
                  probability=dbinom(nheads, 10, .5))
ggplot(flip_df, aes(x=nheads, y=probability)) + 
  geom_bar(stat='identity')
```

```{r plotdbinom-check}
grade_code(incorrect='Try again...')
```


### Cumulative probability distributions

More often, what we want to know is not the probability of a particular value, but rather the probability of a value at least as extreme as the one we have observed.  In this example, let's say that we roll ten coins and observe three heads.  If we are trying to decide whether our coin is fair or not, we care less about the probability of exactly three heads, and more about the probability of *3 or fewer* heads.  This is what we refer to as a *cumulative probability distribution*, which which we can compute using the `pbinom()` function, which takes three arguments:

- the number of "successes" (in this case, the number of heads)
- the number of "trials" (in this case, the number of coin flips)
- the probability of the event occurring (in this case, 0.5 based on our assumption that the coin is fair)


#### Exercise

Plot the results of `pbinom()` for all possible numbers of heads, using `geom_line(stat='identity')`.  First, add a new variable to the *flip_df* data frame called *cumulative_prob* that contains the results from `pbinom()` applied to each value of *nheads*.  Then plot these using a line plot, with `geom_line(stat='identity')`.  The starter code also includes a command to set the tick marks on the x axis (using `scale_x_continuous()`).

```{r pbinomplot, exercise=TRUE, exercise.setup='binom_setup'}
flip_df <- mutate(flip_df, 
                  cumulative_prob=pbinom(nheads, 10, .5))
ggplot(flip_df, aes(x=nheads, y=cumulative_prob)) + 
  geom_line(stat='identity') + 
  scale_x_continuous(breaks=seq(0, 10))
```

This shows us the *lower tail* probability -- that is, the 


```{r pbinom, exercise=TRUE}

p_3_or_fewer_heads <- pbinom(3, 10, 0.5)
p_3_or_fewer_heads
```

This tells us that if we were to repeat our experiment with 10 flips many times, we would expect to see 3 or fewer heads about 17% of time, which is not *that* uncommon. 

### An example
The Astra-Zeneca clinical trial of the COVID-19 vaccine, which aimed to enroll 30,000 patients, was halted on Sept 6, 2020 after one of the patients in the study developed a serious condition known as *transverse myelitis*, which attacks the nervous system.  This disorder occurs in roughly 4 in every million people in any year.  What we would like to know is: How likely would it be for one or more of the people in the study to develop this disorder due to random chance (that is, not as a side effect of the vaccine)?

We can use the `pbinom()` function to compute this.  This function takes three arguments:

- the number of "successes" (in this case, the number of patients with the disease)
- the number of "trials" (in this case, the number of patients in the study - we don't actually know how many were enrolled before the study was stopped, so we will just use the full 30,000 as our estimate)
- the probability of the event occurring (based on the natural occurrence rate of 4 per million people)

Because we want to know the probability of 1 or more cases of transverse myelitis (rather than the probability of exactly one case), we must use the *lower.tail=FALSE* argument, so that the function will return the "upper tail" of the distribution.


```{r pbinom, exercise=TRUE}
p_transverse_myelitis <- 4 / 1000000
n_patients <- 30000
n_observed_TM <- 1

pbinom(n_observed_TM, n_patients, p_transverse_myelitis, lower.tail=FALSE)
```

This tells us that the likelihood of one or more people developing transverse myelitis by chance out of 30,000 people is about 0.007, or about seven out of 1000.  This is a relatively small chance, but not completely out of the question.

#### Exercise

A local election is being held in which 48% of the registered voters support candidate Jerry Garcia and 52% support candidate Robert Plant.  The turnout of the election is 500 voters.  What is the likelihood that candidate Garcia will win the election with more 50% of the vote?

```{r election, exercise=TRUE, exercise.setup='survey-setup'}
# probability that someone will vote for Garcia
p_garcia <- ...
# number of voters
n_voters <- ...
# number of votes needed to win
n_voters_to_win <- ...

p_garcia_wins <- ...
p_garcia_wins

```

```{r election-solution}
p_garcia <- 0.48
n_voters <- 500
n_voters_to_win <- 251

p_garcia_wins <- pbinom(n_voters_to_win, n_voters, p_garcia, lower.tail=FALSE)
p_garcia_wins
```

```{r election-check}
grade_code(incorrect='Try again...')
```
